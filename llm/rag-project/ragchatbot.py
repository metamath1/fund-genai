import random, os, time
from io import BytesIO
import streamlit as st

from dotenv import load_dotenv
import faiss

from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
from langchain_core.prompts import ChatPromptTemplate
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter, KonlpyTextSplitter


from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains import create_retrieval_chain
from langchain.chains import create_history_aware_retriever
from langchain_community.vectorstores import FAISS
from langchain_community.docstore.in_memory import InMemoryDocstore

from langchain_core.prompts import MessagesPlaceholder
# from langchain_core.messages import AIMessage, HumanMessage

# streamlitì„ ì‚¬ìš©í•˜ì—¬ ì±—ë´‡ì„ ë§Œë“œëŠ” íŠœí† ë¦¬ì–¼ ì½”ë“œ
# https://docs.streamlit.io/develop/tutorials/llms/build-conversational-apps

# ì´ ìŠ¤í¬ë¦½íŠ¸ í…ŒìŠ¤íŠ¸ í™˜ê²½ 
# windows@metamath st2 conda í™˜ê²½ì—ì„œ í…ŒìŠ¤íŠ¸ ë¨

#############################################################################
# ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ íƒ¬í”Œë¦¿ 
# ì´ í”„ë¡¬í”„íŠ¸ë¥¼ ì¨ì„œ pdfíŒŒì¼ì„ ì—…ë¡œë“œí•˜ê³  ê°™ì´ ì´ì•¼ê¸° í•  ìˆ˜ ìˆëƒê³  ë¬¼ìœ¼ë©´
# ì•„ë˜ì²˜ëŸ¼ ê°€ëŠ¥í•˜ë‹¤ê³  ëŒ€ë‹µí•˜ê³  ì™¼ìª½ íŒ¨ë„ì—ì„œ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ë¼ê³  ì´ì•¼ê¸°í•¨
system_prompt = (
    "You are an assistant for question-answering tasks. "
    "You can have a conversation based on a PDF file."
    "When a user asks to chat using a PDF file, reply in Korean like this example"
    "-----------------"
    "ë„¤ ê°€ëŠ¥í•©ë‹ˆë‹¤! ìš°ì„  í•¨ê»˜ ëŒ€í™” ë‚˜ëˆŒ pdfë¥¼ ì—…ë¡œë“œ í•´ì£¼ì„¸ìš”. ì—…ë¡œë“œëŠ” ì™¼ìª½ íŒ¨ë„ì—ì„œ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
    "-----------------"

    "Use the following pieces of retrieved context to answer "
    "the question. If you don't know the answer, say that you "
    "don't know. Use three sentences maximum and keep the "
    "answer concise."
    "\n\n"
    "{context}"
)

# ë©”ëª¨ë¦¬ë¥¼ ê°€ì§€ëŠ” ë¦¬íŠ¸ë¦¬ë²„ë¥¼ ìœ„í•œ ë¶€ë¶„
# ì±„íŒ…ì´ ë°˜ë³µë˜ë©´ì„œ ë§ˆì§€ë§‰ì— ì‚¬ìš©ìì˜ ì§ˆë¬¸ì´ ë“¤ì–´ì˜¤ë©´
# ê·¸ ë“¤ì–´ì˜¨ ì§ˆë¬¸ì„ 
# ê³¼ê±° ëŒ€í™” ì´ë ¥ì„ ì°¸ê³ í•˜ì—¬ ê·¸ ì´ë ¥ì„ ëª¨ë‘ í¬í•¨ í•  ìˆ˜ ìˆëŠ” ë‹¤ì‹œë§í•´
# ê³¼ê±° ëŒ€í™” ë§¥ë½ì„ íŒŒì•…í•  ìˆ˜ ìˆëŠ” ì§ˆë¬¸ìœ¼ë¡œ ì¬ìƒì„±í•˜ë¼ëŠ” í”„ë¡¬í”„íŠ¸
# ì´ë ‡ê²Œ ë§¥ë½ì„ í¬í•¨í•œ ì§ˆë¬¸ì„ ì¬ìƒì„±í•´ì„œ ì¬ìƒì„±ëœ ì§ˆë¬¸ê³¼
# ê´€ë ¨ì´ ìˆëŠ” ë¬¸ì„œ ì²­í¬ë“¤ì„ ê²€ìƒ‰í•´ì˜¬ ëª©ì ìœ¼ë¡œ ì‚¬ìš©ë¨ 

# ì›ë˜ í”„ë¡¬í”„íŠ¸
# contextualize_q_system_prompt = """Given a chat history and the latest user question \
# which might reference context in the chat history, formulate a standalone question \
# which can be understood without the chat history. Do NOT answer the question, \
# just reformulate it if needed and otherwise return it as is."""
contextualize_q_system_prompt = """Given a chat history and the latest user question \
which might reference context in the chat history, formulate a standalone question \
which can be understood without the chat history.

!!!CRITICAL INSTRUCTION!!!
YOUR ONLY TASK IS TO REFORMULATE THE QUESTION IF NEEDED.

- DO NOT provide any answers or explanations
- DO NOT offer assistance or suggestions
- DO NOT engage in discussion
- ONLY output either:
    1. The original question (if it's already standalone) OR
    2. A reformulated standalone version of the question

Response MUST BE a question.
"""


contextualize_q_prompt_tpl = ChatPromptTemplate.from_messages(
    [
        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
        ("system", contextualize_q_system_prompt),

        # ì§€ê¸ˆê¹Œì§€ ì±„íŒ… íˆìŠ¤í† ë¦¬
        MessagesPlaceholder("chat_history"),

        # ìƒˆë¡œ ì£¼ì–´ì§„ ì‚¬ìš©ìì˜ ìš”ì²­(ì§ˆë¬¸)
        ("human", "{input}"),
    ]
)
#############################################################################

# ì—…ë¡œë“œëœ pdf_docsì— ìˆëŠ” pdfíŒŒì¼ì„ PyPDFLoaderë¥¼ ì´ìš©í•´ì„œ ë¬¸ì„œë¥¼
# ë­ì²´ì¸ Document ê°ì²´ í˜•íƒœë¡œ ë¡œë”©í•˜ëŠ” í•¨ìˆ˜
def get_pdf_text(pdf_docs):
    pages = []
    
    # ì—…ë¡œë“œëœ íŒŒì¼ í•˜ë‚˜í•˜ë‚˜ì— ëŒ€í•´ì„œ forë£¨í”„ë¥¼ ëŒë©´ì„œ...
    for uploaded_pdf in pdf_docs: # uploaded_pdfëŠ” streamlitì˜ UploadedFile ê°ì²´
        print(f"{uploaded_pdf.name}ë¡œ ë¶€í„° í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œì‘")
        
        # streamlitì€ ì—…ë¡œë“œí•œ íŒŒì¼ì„ ë©”ëª¨ë¦¬ì—ë§Œ ì €ì¥í•˜ë¯€ë¡œ ë””ìŠ¤í¬ë¡œ ì €ì¥í•˜ê³ 
        # langchain PyPDFLoaderë¡œ ì½ìŒ
        save_path = os.path.join("uploaded_files", uploaded_pdf.name)

        with open(save_path, "wb") as f:
            f.write(uploaded_pdf.read())
            print(f"{save_path}ì— íŒŒì¼ ì €ì¥ ì™„ë£Œ")

        # ì €ì¥ëœ íŒŒì¼ì„ ë””ìŠ¤í¬ë¡œ ë¶€í„° ë¡œë”©
        doc_loader = PyPDFLoader(save_path)
        pages += doc_loader.load_and_split()

        print(f"{uploaded_pdf.name}ë¡œ ë¶€í„° í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ")

    return pages
   
# ìœ„ get_pdf_text()ê°€ ë¡œë”©ëœ pdf ë‚´ìš©ì„ ë°˜í™˜í•˜ë©´ ê·¸ê²ƒì„ ì…ë ¥ë°›ì•„
# ë‹¤ì‹œ chunk_sizeë¡œ ìë¥´ëŠ” í•¨ìˆ˜
def get_text_chunks(langchain_docs):

    # text_splitter = RecursiveCharacterTextSplitter(
    text_splitter = KonlpyTextSplitter(
        # separator='.',
        chunk_size=120,  # ì²­í¬(ë¬¸ì„œì˜ ì¡°ê°)ì˜ í¬ê¸°ë¥¼ ì§€ì •
        chunk_overlap=0, # ì˜ë ¤ì§„ ì²­í¬ê°€ ì–¼ë§ˆë‚˜ ê²¹ì¹ ì§€ë¥¼ ì§€ì •
        length_function=len # ë¬¸ì„œë¥¼ ìë¥¼ë•Œ ë¬¸ì„œì˜ ê¸¸ì´ë¥¼ len()ìœ¼ë¡œ íŒë‹¨í•˜ë¼ê³  ì§€ì • 
    )

    # text_splitterë¡œ ë¬¸ì„œë¥¼ ìë¦„ 
    chunks = text_splitter.split_documents(langchain_docs)
    if len(chunks) > 0:
        print("ì „ì²´ pdf ì²­í‚¹ ì™„ë£Œ")

    return chunks

# ìœ„ get_text_chunks()í•¨ìˆ˜ê°€ ì˜ë¼ì¤€ ì‘ì€ ë¬¸ì„œì˜ ì¡°ê°ë“¤ì„
# ë²¡í„°ë¡œ ë³€í™˜í•˜ì—¬ ë°ì´í„° ë² ì´ìŠ¤ì— ì €ì¥í•˜ëŠ” í•¨ìˆ˜
def get_vectorstore(doc_chunks):
    # OpenAIì—ì„œ ì œê³µí•˜ëŠ” ìœ ë£Œ ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ë„ ìˆê³ 
    embeddings = OpenAIEmbeddings(model="text-embedding-3-large")
    # embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
    
    # í—ˆê¹…í˜ì´ìŠ¤ì—ì„œ í˜¸ìŠ¤íŒ…í•˜ëŠ” BERT ëª¨ë¸ì„ ë‹¤ìš´ë°›ì•„ ë¡œì»¬ì—ì„œ ì§ì ‘
    # ì„ë² ë”©í•  ìˆ˜ë„ ìˆìŒ
    # embeddings = HuggingFaceEmbeddings(
    #     model_name='jhgan/ko-sbert-nli',
    #     model_kwargs={'device':'cpu'},
    # )
    
    # pdf íŒŒì¼ì´ ì—…ë¡œë“œë˜ì§€ ì•Šì•„ ë¬¸ì„œ ì²­í¬ë¥¼ ì €ì¥í•œ ë¦¬ìŠ¤íŠ¸ê°€ ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¼ë©´ 
    if not doc_chunks:
        print("doc_chunksê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. ë¹ˆ ë²¡í„° DBë¥¼ ìƒì„±í•©ë‹ˆë‹¤.")
        # ë¹ˆ ë²¡í„° DBë¥¼ ìƒì„±
        vectorstore = FAISS(
            embedding_function=embeddings, 
            index=faiss.IndexFlatL2(len(embeddings.embed_query("hello world"))),
            docstore=InMemoryDocstore(),
            index_to_docstore_id={}
        )
    else:
        # doc_chunksê°€ ë¹„ì–´ìˆì§€ ì•Šìœ¼ë©´ ì •ìƒì ìœ¼ë¡œ ë²¡í„° DB ìƒì„±
        vectorstore = FAISS.from_documents(doc_chunks, embeddings)
    
    print("ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ì™„ë£Œ")
    print(f"ì´ ì €ì¥ëœ ë²¡í„° ìˆ˜: {vectorstore.index.ntotal}")

    return vectorstore

# ìœ„ì—ì„œ ì •ì˜í•œ ê° ë‹¨ê³„ë¥¼ ë‹´ë‹¹í•˜ëŠ” í•¨ìˆ˜ë¥¼ ìˆœì„œëŒ€ë¡œ ì‹¤í–‰í•˜ë©´ì„œ
# ëŒ€í™”ì˜ íˆìŠ¤í† ë¦¬ë¥¼ ë°˜ì˜í•˜ì—¬ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë¬¸ì„œ ì²­í¬ë¥¼ ê²€ìƒ‰í•´ì„œ 
def create_history_aware_rag_chain(pdf_docs):
    ################################################################
    # load documents
    docs_list_from_pdfs = get_pdf_text(pdf_docs)
    ################################################################
    

    ################################################################
    # text splitter
    doc_chunks = get_text_chunks(docs_list_from_pdfs)
    ################################################################


    ################################################################
    # create vector store
    vectorstore = get_vectorstore(doc_chunks)
    ################################################################


    ################################################################
    # create chain

    # ë²¡í„° DBë¥¼ ì²´ì¸ì—ì„œ ì‚¬ìš©í•˜ëŠ” ë¦¬íŠ¸ë¦¬ë²„ë¡œ ë³€ê²½í•˜ê³ 
    retriever = vectorstore.as_retriever(
        search_kwargs={"k": 10} # ê²€ìƒ‰ ì²­í¬ ê°œìˆ˜
    )

    # contextualize_q_prompt_tplì— ì˜í•´ 
    # ê³¼ê±° ëŒ€í™” ë§¥ë½ì„ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ì¬ìƒì„±í•˜ëŠ” ì§ˆë¬¸ì„
    # ì „ë‹¬í•˜ëŠ” llmì´ ë§Œë“¤ê³  ê·¸ ì§ˆë¬¸ê³¼ ê´€ë ¨ìˆëŠ” ë¬¸ì„œ ì²­í¬ë¥¼ retrieverê°€
    # ê²€ìƒ‰í•˜ëŠ” ì—­í• ì„ í•˜ëŠ” ë¦¬íŠ¸ë¦¬ë²„ë¥¼ ìƒì„±
    # https://python.langchain.com/docs/tutorials/qa_chat_history/
    st.session_state.history_aware_retriever = create_history_aware_retriever(
        llm, 
        retriever, contextualize_q_prompt_tpl
    )
    
    # ìµœì¢…ì ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ì²˜ë¦¬í•˜ëŠ” í”„ë¡¬í”„íŠ¸ì™€ ì²´ì¸ì„ ë§Œë“¬
    qa_prompt_tpl = ChatPromptTemplate.from_messages(
        [
            # ì—¬ê¸° system_promptì•ˆì— context ë³€ìˆ˜ê°€ ìˆëŠ”ë°(ìœ„ system_prompt ì •ì˜ ì°¸ê³ )
            # ì´ context ë³€ìˆ˜ì—ëŠ” history_aware_retrieverê°€ ìƒˆë¡­ê²Œ ì§ˆë¬¸ì„ êµ¬ì„±í•˜ê³ 
            # ê²€ìƒ‰í•œ ì²­í¬ë“¤ë¡œ ì±„ì›Œì§€ê²Œ ë¨
            ("system", system_prompt),
            MessagesPlaceholder("chat_history"),
            ("human", "{input}"),
        ]
    )
    question_answer_chain = create_stuff_documents_chain(llm_mini, qa_prompt_tpl)

    # ë§ˆì§€ë§‰ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” ì²´ì¸ê³¼ ëŒ€í™” ë§¥ë½ì„ ë°˜ì˜í•˜ì—¬ ë¬¸ì„œ ì²­í¬ë¥¼ ê²€ìƒ‰í•˜ëŠ”
    # ë¦¬íŠ¸ë¦¬ë²„ë¥¼ ì—°ê²°í•˜ì—¬ ìµœì¢…ì ì¸ rag_chainì„ ë§Œë“¬
    # ë§Œë“¤ì–´ì§„ ì²´ì¸ì„ streamlit ì„¸ì…˜ ë³€ìˆ˜ì— ëŒ€ì…í•˜ì—¬ ì „ì—­ë³€ìˆ˜ì²˜ëŸ¼ ì‚¬ìš©í•  ìˆ˜ ìˆê²Œ ì¤€ë¹„
    st.session_state.rag_chain = create_retrieval_chain(
        st.session_state.history_aware_retriever, 
        question_answer_chain
    )

# ê°„ë‹¨í•˜ê²Œ ê°€ìƒìœ¼ë¡œ ë‹µë³€ì„ ë˜ëŒë¦¬ëŠ” í•¨ìˆ˜
# llmìœ¼ë¡œ ì œëŒ€ë¡œëœ ì²´ì¸ì„ êµ¬ì„±í•˜ê¸° ì „ì— ì „ì²´ ì•±ì„ í…ŒìŠ¤íŠ¸í•´ë³¼ ìš©ë„ë¡œ ë§Œë“¤ì–´ì§
def get_random_msg():
    messages = [
        "ë‚ ì”¨ê°€ ì°¸ ì¢‹êµ°ìš”!",
        "ì–´ì œ í˜¹ì‹œ ì¦ê±°ìš´ ì‡¼í•‘ì„ í•˜ì…¨ë‚˜ìš”?",
        "ì˜¤ëŠ˜ì€ ì¢‹ì€ ì¼ë§Œ ìˆì—ˆìœ¼ë©´ ì¢‹ê² ë„¤ìš”.",
        "ì–¸ì œ í‡´ê·¼í•˜ì„¸ìš”?",
        "í˜¹ì‹œ ì–´ë”” ì•„í”„ì‹ ê°€ìš”?"
    ]

    response = random.choices(messages, k=1)[0]
        
    return response

# ì‚¬ìš©ìë¡œ ë¶€í„° ì…ë ¥ì´ ë“¤ì–´ì˜¤ë©´ ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜
def handle_user_input(user_input):
    # ë¨¼ì € ë””ë²„ê¹… ëª©ì ìœ¼ë¡œ history_aware_retriever ì²´ì¸ì„ invokeí•´ì„œ
    # history_aware_retrieverê°€ ë˜ëŒë¦¬ëŠ” ì²­í¬ë¥¼ í™”ë©´ì„œ ì¶œë ¥í•˜ì—¬ í™•ì¸
    response_foo = st.session_state.history_aware_retriever.invoke(
        {
            "input": user_input,
            "chat_history": st.session_state.messages            
        }
    )
    print("\nhistory_aware_retrieverì˜ ì‘ë‹µ")
    print("CHUNKS--------------------")
    for i, chunk in enumerate(response_foo):
        print(f"{i}ë²ˆì§¸ ì²­í¬: ", chunk.page_content)
        print()
    print("---------------------------")

    # ì‚¬ìš©ì ì…ë ¥ì— ë‹µë³€í•˜ëŠ” ì²´ì¸ì„ ì‘ë™ ì‹œí‚´
    response = st.session_state.rag_chain.invoke(
        {
            "input": user_input,
            "chat_history": st.session_state.messages
        }
    )
    # ì‹¤ì œ llmì²´ì¸ì˜ ì‘ë‹µ responseëŠ” ë‹¤ìŒì²˜ëŸ¼ ìƒê²¼ìŒ
    # {
    #     'input': ë°©ê¸ˆ ë°›ì€ ì§ˆë¬¸
    #     'chat_history': [
    #         {'role':'human', 'content':'ì•ˆë…•í•˜ì„¸ìš”.'},
    #         {'role':'assistant', 'content':'ë¸”ë¼ë¸”ë¼...'},
    #         # ì´ëŸ°ì‹ìœ¼ë¡œ ìœ„ì— ìµœê·¼ ì§ˆë‹µê¹Œì§€ í¬í•¨í•œ ì´ ì§ˆë‹µì˜ ë¦¬ìŠ¤íŠ¸
    #     ],
    #     'answer': ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€
    # }

    return response['answer']

def response_generator(response):
    for word in response.split():
        yield word + " "
        time.sleep(0.05)


if __name__ == '__main__':
    # í™˜ê²½ ë³€ìˆ˜ ë¡œë”©
    load_dotenv()
    # print(os.getenv("OPENAI_API_KEY"))
    # print(os.getenv("HUGGINGFACEHUB_API_TOKEN"))

    # ì—…ë¡œë“œëœ pdf ë‚´ìš©ì„ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
    # pdf ë‚´ìš©ì€ ë­ì²´ì¸ì˜ Document() í˜•ì‹ìœ¼ë¡œ ë¡œë”©ë¨
    docs_list_from_pdfs = []

    #################################################################################
    # LLM ì •ì˜
    # rag ì»¨í…ìŠ¤íŠ¸ì™€ í•¨ê»˜ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•˜ê¸° ìœ„í•´ ì‚¬ìš©
    llm_mini = ChatOpenAI(
        temperature = 0.0,
        model='gpt-4o-mini', 
        # ì´ ì˜µì…˜ì„ ì£¼ë©´ ì½˜ì†”ì— ì‹¤ì‹œê°„ìœ¼ë¡œ ë©”ì„¸ì§€ê°€ ë¿Œë ¤ì§
        # ë‘ê°œì˜ ì²´ì¸ì´ ëŒì•„ê°€ëŠ”ë° ê° ì²´ì¸ì˜ ë‚´ë¶€ ì¶œë ¥ì´ í™”ë©´ì— ë¿Œë ¤ì§€ê²Œ í•  ìˆ˜ ìˆìŒ
        # streaming=True,
        # callbacks=[StreamingStdOutCallbackHandler()]
    )

    # ì—¬ê¸°ì„œëŠ” chat historyë¥¼ ë°˜ì˜í•œ standalone ì§ˆë¬¸ì„ ìƒì„±í•˜ê¸° ìœ„í•´ ì‚¬ìš©
    llm = ChatOpenAI(
        temperature = 0.0,
        model='gpt-4o', 
        streaming=True,
        # ì´ ì˜µì…˜ì„ ì£¼ë©´ ì½˜ì†”ì— ì‹¤ì‹œê°„ìœ¼ë¡œ ë©”ì„¸ì§€ê°€ ë¿Œë ¤ì§
        # ë‘ê°œì˜ ì²´ì¸ì´ ëŒì•„ê°€ëŠ”ë° ê° ì²´ì¸ì˜ ë‚´ë¶€ ì¶œë ¥ì´ í™”ë©´ì— ë¿Œë ¤ì§€ê²Œ í•  ìˆ˜ ìˆìŒ
        callbacks=[StreamingStdOutCallbackHandler()]
    )
    #################################################################################



    ##########################################################################
    # ì‚¬ì´ë“œë°” ì •ì˜ ì˜ì—­
    ##########################################################################
    with st.sidebar:
        st.subheader("ì—…ë¡œë“œëœ PDF ë¬¸ì„œ")

        # pdf_docs: list of pdf files
        pdf_docs = st.file_uploader(
            "PDF íŒŒì¼ë“¤ì„ ì—…ë¡œë“œí•˜ê³  'Process' ë²„íŠ¼ì„ ëˆ„ë¥´ì„¸ìš”.",
            accept_multiple_files=True # ì—¬ëŸ¬ íŒŒì¼ ì—…ë¡œë“œ
        )
        
        #######################################################################
        # ì—…ë¡œë“œëœ pdf íŒŒì¼ ì „ì²˜ë¦¬ ë¶€ë¶„
        if st.button("Process"): # ì‚¬ìš©ìê°€ ë²„íŠ¼ì„ ëˆŒë €ìœ¼ë©´ ë§¤ë²ˆ ìƒˆë¡œìš´ ì²´ì¸ ìƒì„±
            with st.spinner("Processing..."):
                # í˜„ì¬ ì—…ë¡œë“œëœ pdfíŒŒì¼ (pdf_docs)ë¥¼ ë„˜ê¸°ë©´ì„œ ì²´ì¸ ìƒì„±
                create_history_aware_rag_chain(pdf_docs)
        else: 
            # ì„¸ì…˜ ë³€ìˆ˜ì— rag_chainì´ë¼ëŠ” í•­ëª©ì´ ì—†ìœ¼ë©´ ìµœì´ˆë¡œ ì²´ì¸ì„ ìƒì„±
            # ì´ë•ŒëŠ” process ë²„íŠ¼ì„ ëˆ„ë¥´ì§€ ì•Šì•˜ê¸° ë•Œë¬¸ì—
            # pdfíŒŒì¼ì„ ì²˜ë¦¬í•˜ì§€ ì•Šê³  ê·¸ëƒ¥ ì²´ì¸ë§Œ ë§Œë“¬ 
            if "rag_chain" not in st.session_state:
                create_history_aware_rag_chain(pdf_docs)
        #######################################################################
        
        if st.button("chat_history"):
            print('-------chat_history---------')
            print(st.session_state.messages)
            print('----------------------------')
        # debugging pdf_docs 
        # UploadedFile ê°ì²´
        # ì—…ë¡œë“œëœ ê°ì²´ë¥¼ í™”ë©´ì— ë¿Œë¦¼ 
        # print("pdf_docs", pdf_docs)
        # print("docs_list_from_pdfs", docs_list_from_pdfs)
    ##########################################################################
    # ì‚¬ì´ë“œë°” ì •ì˜ ì˜ì—­
    ##########################################################################

    ##########################################################################
    # ë©”ì¸í™”ë©´ ì •ì˜ ì˜ì—­
    ##########################################################################
    st.title("ğŸ” LangChain - RAG")

    # st.session_state['messages'] ëŠ” ì´ ì•±ì´ ë©”ì„¸ì§€ë¥¼ ê¸°ë¡í•˜ê¸° ìœ„í•œ ë²„í¼
    # ê¸°ë¡ë˜ëŠ” ë ˆì½”ë“œ í˜•ì‹ì€ {'role':'', 'content':''} ì‹ì´ê³ 
    # ì²˜ìŒì— ê¸°ë¡ëœê²Œ ì—†ë‹¤ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¡œ ì´ˆê¸°í™”
    if "messages" not in st.session_state:
        st.session_state["messages"] = [ ]

    # st.chat_messageëŠ” í™”ë©´ì— ë¿Œë¦¬ëŠ” ìš©ë„ì˜ ë²„í¼
    # st.chat_message(role)ë¡œ writeë¥¼ í•˜ë©´ ê°€ê° ë¡¤ì— ë§ê²Œ í™”ë©´ì— ë¿Œë ¤ì§
    for msg in st.session_state.messages:
       st.chat_message(msg["role"]).write(msg["content"])

    # st.chat_inputì€ í™”ë©´ ì•„ë˜ ë‚˜íƒ€ë‚˜ëŠ” ìœ ì € ì¸í’‹ì„ ë°›ëŠ” ì…ë ¥ì°½
    if prompt := st.chat_input(placeholder="ì–´ë–¤ ì§ˆë¬¸ì´ë“  í•´ë³´ì„¸ìš”."):
        # ìœ ì € ì¸í’‹ì„ ë©”ì„¸ì§€ íˆìŠ¤í† ë¦¬ì— ì €ì¥í•˜ê³ 
        st.session_state.messages.append({"role": "user", "content": prompt})
        # ë°”ë¡œ í™”ë©´ì— ì¶œë ¥
        st.chat_message("user").write(prompt)

        # ì—¬ê¸°ì„œ ì²´ì¸ ëŒë¦¬ê¸°
        # response = get_random_msg() # ì‹œí—˜ ì‚¼ì•„ ë¬´ì‘ìœ„ ì‘ë‹µ ë°›ì•„ì˜¤ê¸° 
        response = handle_user_input(prompt)

        with st.chat_message("assistant"):
            # llmì˜ ì‘ë‹µì„ ë©”ì„¸ì§€ íˆìŠ¤í† ë¦¬ì— ì €ì¥í•˜ê³  
            st.session_state.messages.append({"role": "assistant", "content": response})
            # ì¶œë ¥, ì—¬ê¸°ì„œëŠ” with ë¬¸ì•ˆì— ìˆê¸° ë•Œë¬¸ì— st.write()ë§Œ í•´ë„
            # st.chat_message("assistant").write()ë¡œ ì‘ë™í•¨
            st.write_stream(response_generator(response))
    ##########################################################################
    # ë©”ì¸í™”ë©´ ì •ì˜ ì˜ì—­        
    ##########################################################################