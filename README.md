# Generative AI Fundamental Course Repository

This repository is designed to provide in-depth knowledge and practical skills in generative AI, including Transformer models, Large Language Models, and Image Generative AI.

This content is part of the **AI Zero to Master** program conducted by **Daegu AI-Hub**.

---

## üìö **Course Contents**

### 1. **Transformer**
- **Deep Dive into Transformer Models**  
  Detailed analysis and understanding of the Transformer architecture.
- **Predicting Simple Sequences with Transformers**  
  Implementing a basic Transformer for sequence prediction tasks.
- **Fine-Tuning GPT-2 for News Headline Generation**  
  Hands-on project to generate news headlines by fine-tuning GPT-2.
- **Fine-Tuning BERT for NSMC Classification**  
  Training BERT on the Naver Sentiment Movie Corpus (NSMC) for sentiment analysis.

---

### 2. **Large Language Models (LLMs)**
- **Key Technologies Leading to LLMs**  
  A review of essential advancements that enabled the development of LLMs.
- **Utilizing OpenAI API and Prompt Engineering**  
  Practical usage of ChatGPT and prompt engineering techniques.
- **LangChain Basics and RAG App Development**  
  Introduction to LangChain and a project for building a Retrieval-Augmented Generation (RAG) application.

---

### 3. **Image Generative AI**
- **Introduction to AutoEncoders and Variational AutoEncoders**  
  Theory and hands-on sessions for understanding AutoEncoders and VAEs.
- **Denoising Diffusion Models**  
  - Overview of Denoising Diffusion Probabilistic Models (DDPM).  
  - Proof-of-Concept (PoC) implementation of unconditional DDPM.  
  - PoC implementation of conditional DDPM.
- **Latent Diffusion Models (LDMs)**  
  - Introduction to LDMs and their applications.  
  - PoC implementation of unconditional and conditional LDMs.
- **HuggingFace Diffusers Framework**  
  - Usage of Diffusers library for generative tasks.  
  - Exploring various Diffusers pipelines and creating an Inpainting App.
- **Stable Diffusion Fine-Tuning**  
  - Comprehensive training of Stable Diffusion.  
  - SD 1.5 Model Full Fine-Tuning
  - LoRA adapter training using PEFT (Parameter Efficient Fine-Tuning).

---


## üìù **License**
This repository is licensed under the MIT License. Feel free to use and adapt the materials for educational purposes.

---

## üìß **Contact**
For questions or further information, please reach out to:  
üì© Email: metamath@gmail.com
üåê Website: [https://metamath1.github.io/blog](https://metamath1.github.io/blog)