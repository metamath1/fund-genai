# Generative AI Fundamental Course Repository

This repository is designed to provide fundametal knowledge and practical skills in generative AI, including Transformer models, Large Language Models, and Image Generative AI.
ì´ ë¦¬í¬ì§€í† ë¦¬ëŠ” íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸, ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸, ì´ë¯¸ì§€ ìƒì„± AI ë“± ìƒì„± AIì˜ ê¸°ì´ˆì ì¸ ì§€ì‹ê³¼ ì‹¤ë¬´ ê¸°ìˆ ì„ ì œê³µí•˜ê¸° ìœ„í•´ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤.

This content is part of the **Zero to AI Master** program conducted by **Daegu AI-Hub**.
ì´ ì½˜í…ì¸ ëŠ” **ëŒ€êµ¬ AI í—ˆë¸Œ**ì—ì„œ ì§„í–‰í•˜ëŠ” **ì œë¡œ íˆ¬ AI ë§ˆìŠ¤í„°** í”„ë¡œê·¸ë¨ì˜ ì¼í™˜ì…ë‹ˆë‹¤.

---

## ğŸ“š **Course Contents**

### 1. **Transformer**
- **Deep Dive into Transformer Models**  
  Detailed analysis and understanding of the Transformer architecture. íŠ¸ëœìŠ¤í¬ë¨¸ ì•„í‚¤í…ì²˜ì— ëŒ€í•œ ìì„¸í•œ ë¶„ì„ê³¼ ì´í•´
- **Predicting Simple Sequences with Transformers**  
  Using torch.nn.Transformer to Predict Simple Sequences. ê°„ë‹¨í•œ ìˆ˜ì—´ì„ ì˜ˆì¸¡í•˜ê¸° ìœ„í•œ torch.nn.Transformer ì‚¬ìš©ë²• 
- **Fine-Tuning GPT-2 for News Headline Generation**  
  Hands-on project to generate news headlines by fine-tuning GPT-2. GPT-2ë¥¼ ë¯¸ì„¸ ì¡°ì •í•˜ì—¬ ë‰´ìŠ¤ í—¤ë“œë¼ì¸ì„ ìƒì„±í•˜ëŠ” ì‹¤ìŠµ í”„ë¡œì íŠ¸
- **Fine-Tuning BERT for NSMC Classification**  
  Hands-on with fine-tuning BERT with Naver Sentiment Movie Corpus (NSMC) ë„¤ì´ë²„ ê°ì„± ë¬´ë¹„ ì½”í¼ìŠ¤(NSMC)ë¡œ BERTë¥¼ íŒŒì¸íŠœë‹í•˜ëŠ” ì‹¤ìŠµ

---

### 2. **Large Language Models (LLMs)**
- **Key Technologies Leading to LLMs**  
  A review of essential advancements that enabled the development of LLMs. LLMì˜ ë°œì „ì„ ê°€ëŠ¥í•˜ê²Œ í•œ í•„ìˆ˜ì ì¸ ê³¼ì • ëŒ€í•œ ë¦¬ë·°
- **Utilizing OpenAI API and Prompt Engineering**  
  Practical usage of ChatGPT and prompt engineering techniques. ChatGPT APIì˜ ì‹¤ì œ ì‚¬ìš©ë²•ê³¼ í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ ê¸°ë²•.
- **LangChain Basics and RAG App Development**  
  Introduction to LangChain and a project for building a Retrieval-Augmented Generation (RAG) application. LangChain ì†Œê°œ ë° ê²€ìƒ‰ ì¦ê°• ì„¸ëŒ€(RAG) ì• í”Œë¦¬ì¼€ì´ì…˜ êµ¬ì¶• í”„ë¡œì íŠ¸
  <div style="display: flex; flex-wrap: wrap; justify-content: space-around; text-align: center;">
    <div style="margin: 10px;">
      <img src="llm/rag-project/chat_wo_rag.png" alt="without RAG" width="450"/>
      <p>without RAG</p>
    </div>
    <div style="margin: 10px;">
      <img src="llm/rag-project/chat_rag.png" alt="with RAG" width="450"/>
      <p>with RAG</p>
    </div>
  </div>

---

### 3. **Image Generative AI**
- **Introduction to AutoEncoders and Variational AutoEncoders**  
  Theory and hands-on sessions for understanding AutoEncoders and VAE. ìë™ ì¸ì½”ë”ì™€ VAEë¥¼ ì´í•´í•˜ê¸° ìœ„í•œ ì´ë¡  ë° ì‹¤ìŠµ
- **Denoising Diffusion Models**  
  - Overview of Denoising Diffusion Probabilistic Models (DDPM). ë…¸ì´ì¦ˆ ì œê±° í™•ì‚° í™•ë¥  ëª¨ë¸(DDPM) ê°œìš”
  - Proof-of-Concept (PoC) implementation of unconditional DDPM. ë¬´ì¡°ê±´ DDPMì˜ ê°œë… ì¦ëª…(PoC) êµ¬í˜„.  
  - PoC implementation of conditional DDPM. ì¡°ê±´ë¶€ DDPMì˜ ê°œë… ì¦ëª…(PoC) êµ¬í˜„.  
- **Latent Diffusion Models (LDMs)**  
  - Introduction to LDMs and their applications. LDMê³¼ ê·¸ í™œìš©ì— ëŒ€í•œ ì†Œê°œ
  - PoC implementation of unconditional and conditional LDM. ë¬´ì¡°ê±´ ë° ì¡°ê±´ë¶€ LDMì˜ PoC êµ¬í˜„
- **HuggingFace ğŸ¤—Diffusers Framework**  
  - Introducing ğŸ¤—Diffusers Library for Image Generation Tasks. ì´ë¯¸ì§€ ìƒì„± ì‘ì—…ì„ ìœ„í•œ ë””í“¨ì € ë¼ì´ë¸ŒëŸ¬ë¦¬ ì†Œê°œ
  - Training an Image Generation Model with ğŸ¤—Diffusers. ë””í“¨ì € ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì´ìš©í•œ ì´ë¯¸ì§€ ìƒì„± ëª¨ë¸ í•™ìŠµ
  - Exploring different ğŸ¤—Diffusers pipelines and building an in-painting app project. ë‹¤ì–‘í•œ ë””í“¨ì € íŒŒì´í”„ë¼ì¸ì„ íƒìƒ‰ ë° ì¸í˜ì¸íŒ… ì•± ì œì‘ í”„ë¡œì íŠ¸ 
 
  | without Prompt            | with Prompt                    |
  |---------------------------|--------------------------------|
  | ![Base](image-genai/inpaint-project/null_prompt.png) | ![Inpainted](image-genai/inpaint-project/prompt.png) |
  | None |  A small robot, high resolution, sitting on a park bench |

- **Stable Diffusion Fine-Tuning**  
  - SD 1.5 Model Full Fine-Tuning ìŠ¤í…Œì´ë¸” ë””í“¨ì „ 1.5 í’€ íŒŒì¸íŠœë‹
  - LoRA adapter training using PEFT (Parameter Efficient Fine-Tuning). ìŠ¤í…Œì´ë¸” ë””í“¨ì „ 1.5 LoRAë¥¼ ì´ìš©í•œ íŒŒì¸íŠœë‹
    <div style="display: flex; flex-wrap: wrap; justify-content: space-around; text-align: center;">
    <div style="margin: 10px;">
      <img src="image-genai/diffusers/fine-tuning.png" alt="SD 1.5 fine-tuning" width="950"/>
      <p>without RAG</p>
    </div>
  </div>
---


## ğŸ“ **License**
This repository is licensed under the MIT License. Feel free to use and adapt the materials for educational purposes.

---

## ğŸ“§ **Contact**
For questions or further information, please reach out to:  
ğŸ“© Email: metamath@gmail.com
ğŸŒ Website: [https://metamath1.github.io/blog](https://metamath1.github.io/blog)